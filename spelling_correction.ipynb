{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#词典库  vocab.txt 为词库\n",
    "vocab = {line.rstrip() for line in open('./data/vocab.txt',encoding='utf-8')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#生成编辑距离为2的候选集合\n",
    "def generate_candidates_two(word_one):\n",
    "    letters = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    candidates_two = []\n",
    "    for word in word_one:\n",
    "        splits = [(word[:i] , word[i:]) for i in range(len(word) + 1)]\n",
    "        #insert操作\n",
    "        inserts = [L+c+R for L,R in splits for c in letters]\n",
    "        #repalce操作\n",
    "        replaces = [L+c+R[1:] for L,R in splits if R for c in letters]\n",
    "        #delete操作\n",
    "        deletes = [L+R[1:] for L,R in splits if R]\n",
    "    \n",
    "        candidates_two += set(inserts + replaces + deletes)\n",
    "    return [word for word in candidates_two if word in vocab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#需要生成所有的候选集合\n",
    "def generate_candidates(word):\n",
    "    \"\"\"\n",
    "    word:给定的输入(错误的输入)\n",
    "    返回所有(valid)候选集合\n",
    "    \"\"\"\n",
    "    #生成编辑距离为1的单词\n",
    "    # 1.insert 2.delete 3.replace\n",
    "    #假设使用26个字符\n",
    "    letters = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    \n",
    "    splits = [(word[:i] , word[i:]) for i in range(len(word) + 1)]\n",
    "    #insert操作\n",
    "    inserts = [L+c+R for L,R in splits for c in letters]\n",
    "    #repalce操作\n",
    "    replaces = [L+c+R[1:] for L,R in splits if R for c in letters]\n",
    "    #delete操作\n",
    "    deletes = [L+R[1:] for L,R in splits if R]\n",
    "    \n",
    "    candidate_one = set(inserts + replaces + deletes)\n",
    "    #过滤掉不存在词典库中的单词\n",
    "    return [word for word in candidate_one if word in vocab]\n",
    "#     word_one =  [word for word in candidate_one if word in vocab]\n",
    "#     if len(word_one) < 1:\n",
    "#         return generate_candidates_two(candidate_one)\n",
    "#     else:\n",
    "#         return [word for word in candidate_one if word in vocab]\n",
    "#generate_candidates(\"appc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import reuters #路透社语料库\n",
    "#读取语料库\n",
    "categories = reuters.categories()\n",
    "corpus = reuters.sents(categories = categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 构建语言模型 unigram bigram trigram\n",
    "term_count = {} #单个单词出现的次数\n",
    "bigram_count = {} #连续两个单词出现的次数\n",
    "for doc in corpus:\n",
    "    doc = ['<s>'] + doc\n",
    "    for i in range(0,len(doc) - 1):\n",
    "        #bigram: [i, i+1]\n",
    "        term = doc[i]\n",
    "        bigram = doc[i:i+2]\n",
    "        \n",
    "        if term in term_count:\n",
    "            term_count[term] += 1\n",
    "        else:\n",
    "            term_count[term] = 1\n",
    "        bigram = ' '.join(bigram)\n",
    "        if bigram in bigram_count:\n",
    "            bigram_count[bigram] += 1\n",
    "        else:\n",
    "            bigram_count[bigram] = 1\n",
    "# sklearn里面有现成的包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#用户打错的概率统计 - channel probability    \n",
    "channel_prob = {} #就是一个二维元组，第一维是正确单词，第二维是错误单词\n",
    "for line in open(\"./data/spell-errors.txt\"):\n",
    "    items = line.split(\":\")\n",
    "    correct = items[0].strip()\n",
    "    mistakes = [word.strip() for word in items[1].strip().split(\",\")]\n",
    "    channel_prob[correct] = {}\n",
    "    for mis in mistakes:\n",
    "        channel_prob[correct][mis] = 1.0 / len(mistakes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "V = len(term_count.keys()) #smothing操作中,单词的总数V\n",
    "file = open(\"./data/testdata.txt\",'r')\n",
    "\n",
    "for line in file:\n",
    "    items = line.rstrip().split('\\t') # items 是一个三维数组， items[2]是一个句子\n",
    "    line = items[2].split()\n",
    "    line = ['<s>'] + line  #把获取的一句话，进行与计算bigram_count相同的预处理\n",
    "    for word in line:\n",
    "        if word not in vocab: \n",
    "            #如果句子中，有单词不在词典中    word:错误的单词\n",
    "            #step1：生成所有的(valid)候选集合\n",
    "            candidates = generate_candidates(word)\n",
    "            if len(candidates) < 1:\n",
    "                continue\n",
    "            probs = []\n",
    "            #对于每一个candidate，计算它的score\n",
    "            # score = p(correct) * p(mistake|correct)\n",
    "            #       = log p(correct) + log p(mistake|correct)\n",
    "            #返回score最大的candidate\n",
    "            for candi in candidates:\n",
    "                prob = 0\n",
    "                #计算 channel概率  出错的概率   \n",
    "                if candi in channel_prob and word in channel_prob[candi]:#正确单词在channel_prob中，并且也存在对应错误写法\n",
    "                    prob += np.log(channel_prob[candi][word])\n",
    "                else:\n",
    "                    prob += np.log(0.00001)\n",
    "                # 计算语言模型的概率\n",
    "                idx = line.index(word) #找到错误单词，在句子中的位置\n",
    "                #计算idx位置左边的bigram_prob\n",
    "                left_bigram = line[idx-1] +' ' + candi\n",
    "                if left_bigram in bigram_count:#判断bigram 是否在bigtam_count中\n",
    "                    prob += np.log(bigram_count[left_bigram] + 1.0 / (term_count[candi] + V))\n",
    "                else:\n",
    "                    prob += np.log(1.0 / V)\n",
    "                #计算idx位置右边的bigram_prob\n",
    "                if idx != len(line)-1:\n",
    "                    right_bigram = candi + ' '+ line[idx+1]\n",
    "                    if right_bigram in bigram_count:\n",
    "                        prob += np.log(bigram_count[right_bigram] + 1.0 /(term_count[candi] + V))\n",
    "                probs.append(prob)\n",
    "            max_index = probs.index(max(probs))\n",
    "            #print(word,candidates[max_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e\n",
      "e\n"
     ]
    }
   ],
   "source": [
    "items = 'hello world'\n",
    "line = items.strip().split()\n",
    "print(items[1])\n",
    "line = items.rstrip()\n",
    "print(line[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
